{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating COVID-19's $R_t$ in Real-Time with PYMC3\n",
    "\n",
    "\n",
    "This notebook is based off of the fantastic work of [Kevin Systrom](https://github.com/k-sys) & the fine folks at [rt.live](https://rt.live)\n",
    "\n",
    "The model itself is based on LuÃ­s M. A. Bettencourt and Ruy M. Ribeiro's [paper](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0002185)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import pymc3 as pm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import theano.tensor.slinalg\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import dates as mdates\n",
    "from matplotlib import ticker\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "import time\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils.load_data import load_population_df, load_confirmed_cases_df \n",
    "\n",
    "import subprocess\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load State Information\n",
    "#### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://covidtracking.com/api/v1/states/daily.csv'\n",
    "states = pd.read_csv(url,\n",
    "                     parse_dates=['date'],\n",
    "                     index_col=['state', 'date']).sort_index()\n",
    "\n",
    "# Note: GU/AS/VI do not have enough data for this model to run\n",
    "# Note: PR had -384 change recently in total count so unable to model\n",
    "states = states.drop(['MP', 'GU', 'AS', 'PR', 'VI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean data with known modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errors in Covidtracking.com\n",
    "states.loc[('WA','2020-04-21'), 'positive'] = 12512\n",
    "states.loc[('WA','2020-04-22'), 'positive'] = 12753 \n",
    "states.loc[('WA','2020-04-23'), 'positive'] = 12753 + 190\n",
    "\n",
    "states.loc[('VA', '2020-04-22'), 'positive'] = 10266\n",
    "states.loc[('VA', '2020-04-23'), 'positive'] = 10988\n",
    "\n",
    "states.loc[('PA', '2020-04-22'), 'positive'] = 35684\n",
    "states.loc[('PA', '2020-04-23'), 'positive'] = 37053\n",
    "\n",
    "states.loc[('MA', '2020-04-20'), 'positive'] = 39643\n",
    "\n",
    "states.loc[('CT', '2020-04-18'), 'positive'] = 17550\n",
    "states.loc[('CT', '2020-04-19'), 'positive'] = 17962\n",
    "\n",
    "states.loc[('HI', '2020-04-22'), 'positive'] = 586\n",
    "\n",
    "states.loc[('RI', '2020-03-07'), 'positive'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_case_pop_df(\n",
    "                population_file_path, \n",
    "                case_file_path, \n",
    "                cum_cases_col='cases',\n",
    "                date_col='date',\n",
    "                pop_fips_col='fips',\n",
    "                case_fips_col='countyFIPS',\n",
    "                case_county_col='County Name',\n",
    "                case_state_col='state'\n",
    "    \n",
    "      ):\n",
    "    '''\n",
    "    A dirty custom function designed to aid the consolidation of population data\n",
    "    against case data for a certain geography.\n",
    "    '''\n",
    "    # Population Data at county level\n",
    "    pop_df = load_population_df(population_file_path)\n",
    "    print(pop_df.shape)\n",
    "\n",
    "    # COVID Cases\n",
    "    cases_df = load_confirmed_cases_df(case_file_path)\n",
    "    print(cases_df.shape)\n",
    "\n",
    "\n",
    "    ##############################################################\n",
    "\n",
    "    cases_pop_df = pd.merge(\n",
    "        left=cases_df,\n",
    "        right=pop_df.rename(columns={pop_fips_col:case_fips_col}),\n",
    "        left_on=case_fips_col,\n",
    "        right_on=case_fips_col,\n",
    "        how='left'\n",
    "    ).drop_duplicates()\n",
    "\n",
    "\n",
    "    cases_pop_df['County_State'] = cases_pop_df[case_county_col].str.title()\\\n",
    "                + ' ' + cases_pop_df[case_state_col].str.upper()\n",
    "    # cases_pop_df['active_cases'] = cases_pop_df['cases'] - cases_pop_df['cases'].shift(14).fillna(0)\n",
    "    # cases_pop_df['new_cases'] = cases_pop_df['cases'].diff()\n",
    "\n",
    "    ##############################################################\n",
    "\n",
    "\n",
    "    append_list = []\n",
    "    for n, g in cases_pop_df.groupby('County_State'):\n",
    "        g.sort_values(date_col, inplace=True)\n",
    "        g['new_cases'] = g[cum_cases_col].diff()\n",
    "        g['active_cases'] = g[cum_cases_col] - g[cum_cases_col].shift(14).fillna(0)\n",
    "        append_list.append(g)\n",
    "    cases_pop_df = pd.concat(append_list)\n",
    "    del append_list\n",
    "    return cases_pop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_pop_df = create_case_pop_df(\n",
    "    population_file_path='../data/misc/CountyHealthRankings19.csv',\n",
    "    case_file_path='../data/county_level/covid_confirmed_usafacts.csv'\n",
    ")\n",
    "\n",
    "cases_pop_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Patient Information\n",
    "\n",
    "Data for the section below is sourced from the University of Washington's [Outbreak and Pandemic Preparedness team](https://github.com/beoutbreakprepared/nCoV2019/raw/master/latest_data/latestdata.tar.gz)\n",
    "\n",
    "This allows for tracking symptoms vs positive case results, thus giving a better sense of likely onset dates vs dates reported in the official counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINELIST_PATH = '../data/misc/latestdata.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Probability Distribution of Delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_p_delay(onset_confirm_path=LINELIST_PATH):\n",
    "\n",
    "    # Load the patient CSV\n",
    "    patients = pd.read_csv(\n",
    "        LINELIST_PATH,\n",
    "        parse_dates=False,\n",
    "        usecols=[\n",
    "            'date_confirmation',\n",
    "            'date_onset_symptoms'],\n",
    "        low_memory=False)\n",
    "\n",
    "    patients.columns = ['Onset', 'Confirmed']\n",
    "\n",
    "    # There's an errant reversed date\n",
    "    patients = patients.replace('01.31.2020', '31.01.2020')\n",
    "\n",
    "    # Only keep if both values are present\n",
    "    patients = patients.dropna()\n",
    "\n",
    "    # Must have strings that look like individual dates\n",
    "    # \"2020.03.09\" is 10 chars long\n",
    "    is_ten_char = lambda x: x.str.len().eq(10)\n",
    "    patients = patients[is_ten_char(patients.Confirmed) & \n",
    "                        is_ten_char(patients.Onset)]\n",
    "\n",
    "    # Convert both to datetimes\n",
    "    patients.Confirmed = pd.to_datetime(\n",
    "        patients.Confirmed, format='%d.%m.%Y', errors='coerce')\n",
    "    patients.Onset = pd.to_datetime(\n",
    "        patients.Onset, format='%d.%m.%Y', errors='coerce')\n",
    "\n",
    "    # Only keep records where confirmed > onset\n",
    "    patients = patients[patients.Confirmed >= patients.Onset]\n",
    "    \n",
    "    \n",
    "\n",
    "    # Calculate the delta in days between onset and confirmation\n",
    "    delay = (patients.Confirmed - patients.Onset).dt.days\n",
    "\n",
    "    # Convert samples to an empirical distribution\n",
    "    p_delay = delay.value_counts().sort_index()\n",
    "    new_range = np.arange(0, p_delay.index.max()+1)\n",
    "    p_delay = p_delay.reindex(new_range, fill_value=0)\n",
    "    p_delay /= p_delay.sum()\n",
    "    \n",
    "    return p_delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_delay = calc_p_delay(LINELIST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = patients.plot.scatter(\n",
    "    title='Onset vs. Confirmed Dates - COVID19',\n",
    "    x='Onset',\n",
    "    y='Confirmed',\n",
    "    alpha=.1,\n",
    "    lw=0,\n",
    "    s=10,\n",
    "    figsize=(6,6))\n",
    "\n",
    "formatter = mdates.DateFormatter('%m/%d')\n",
    "locator = mdates.WeekdayLocator(interval=2)\n",
    "\n",
    "for axis in [ax.xaxis, ax.yaxis]:\n",
    "    axis.set_major_formatter(formatter)\n",
    "    axis.set_major_locator(locator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $R_t$ estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate Confirmation Dates to Onset Dates\n",
    "\n",
    "Our goal is to translate positive test counts to the dates where they likely occured. Since we have the distribution, we can distribute case counts back in time according to that distribution. To accomplish this, we reverse the case time series, and convolve it using the distribution of delay from onset to confirmation. Then we reverse the series again to obtain the onset curve. Note that this means the data will be 'right censored' which means there are onset cases that have yet to be reported so it looks as if the count has gone down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confirmed_to_onset(confirmed, p_delay):\n",
    "\n",
    "    assert not confirmed.isna().any()\n",
    "    \n",
    "    # Reverse cases so that we convolve into the past\n",
    "    convolved = np.convolve(confirmed[::-1].values, p_delay)\n",
    "\n",
    "    # Calculate the new date range\n",
    "    dr = pd.date_range(end=confirmed.index[-1],\n",
    "                       periods=len(convolved))\n",
    "\n",
    "    # Flip the values and assign the date range\n",
    "    onset = pd.Series(np.flip(convolved), index=dr)\n",
    "    \n",
    "    return onset\n",
    "\n",
    "\n",
    "# onset = confirmed_to_onset(confirmed, p_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust for Right-Censoring\n",
    "\n",
    "Since we distributed observed cases into the past to recreate the onset curve, we now have a right-censored time series. We can correct for that by asking what % of people have a delay less than or equal to the time between the day in question and the current day.\n",
    "\n",
    "For example, 5 days ago, there might have been 100 cases onset. Over the course of the next 5 days some portion of those cases will be reported. This portion is equal to the cumulative distribution function of our delay distribution. If we know that portion is say, 60%, then our current count of onset on that day represents 60% of the total. This implies that the total is 166% higher. We apply this correction to get an idea of what actual onset cases are likely, thus removing the right censoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_onset_for_right_censorship(onset, p_delay):\n",
    "    cumulative_p_delay = p_delay.cumsum()\n",
    "    \n",
    "    # Calculate the additional ones needed so shapes match\n",
    "    ones_needed = len(onset) - len(cumulative_p_delay)\n",
    "    padding_shape = (0, ones_needed)\n",
    "    \n",
    "    # Add ones and flip back\n",
    "    cumulative_p_delay = np.pad(\n",
    "        cumulative_p_delay,\n",
    "        padding_shape,\n",
    "        constant_values=1)\n",
    "    cumulative_p_delay = np.flip(cumulative_p_delay)\n",
    "    \n",
    "    # Adjusts observed onset values to expected terminal onset values\n",
    "    adjusted = onset / cumulative_p_delay\n",
    "    \n",
    "    return adjusted, cumulative_p_delay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the Posterior with PyMC3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume a poisson likelihood function and feed it what we believe is the onset curve based on reported data. We model this onset curve based on the math in the Bettencourt & Ribeiro [paper](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0002185):\n",
    "\n",
    "$$ I^\\prime = Ie^{\\gamma(R_t-1)} $$\n",
    "\n",
    "We define $\\theta = \\gamma(R_t-1)$ and model $ I^\\prime = Ie^{\\theta} $ where $\\theta$ observes a random walk. We let $\\gamma$ vary independently based on known parameters for the serial interval. Therefore, we can recover $R_t$ easily by $R_t = \\frac{\\theta}{\\gamma}+1$\n",
    "\n",
    "The only tricky part is understanding that we're feeding in _onset_ cases to the likelihood. So $\\mu$ of the poisson is the positive, non-zero, expected onset cases we think we'd see today.\n",
    "\n",
    "We calculate this by figuring out how many cases we'd expect there to be yesterday total when adjusted for bias and plugging it into the first equation above. We then have to re-bias this number back down to get the expected amount of onset cases observed that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCMCModel(object):\n",
    "    \n",
    "    def __init__(self, region, onset, cumulative_p_delay, window=50):\n",
    "        \n",
    "        # Just for identification purposes\n",
    "        self.region = region\n",
    "        \n",
    "        # For the model, we'll only look at the last N\n",
    "        self.onset = onset.iloc[-window:]\n",
    "        self.cumulative_p_delay = cumulative_p_delay[-window:]\n",
    "        \n",
    "        # Where we store the results\n",
    "        self.trace = None\n",
    "        self.trace_index = self.onset.index[1:]\n",
    "\n",
    "    def run(self, chains=1, tune=3000, draws=1000, target_accept=.95):\n",
    "\n",
    "        with pm.Model() as model:\n",
    "\n",
    "            # Random walk magnitude\n",
    "            step_size = pm.HalfNormal('step_size', sigma=.03)\n",
    "\n",
    "            # Theta random walk\n",
    "            theta_raw_init = pm.Normal('theta_raw_init', 0.1, 0.1)\n",
    "            theta_raw_steps = pm.Normal('theta_raw_steps', shape=len(self.onset)-2) * step_size\n",
    "            theta_raw = tt.concatenate([[theta_raw_init], theta_raw_steps])\n",
    "            theta = pm.Deterministic('theta', theta_raw.cumsum())\n",
    "\n",
    "            # Let the serial interval be a random variable and calculate r_t\n",
    "            serial_interval = pm.Gamma('serial_interval', alpha=6, beta=1.5)\n",
    "            gamma = 1.0 / serial_interval\n",
    "            r_t = pm.Deterministic('r_t', theta/gamma + 1)\n",
    "\n",
    "            inferred_yesterday = self.onset.values[:-1] / self.cumulative_p_delay[:-1]\n",
    "            \n",
    "            expected_today = inferred_yesterday * self.cumulative_p_delay[1:] * pm.math.exp(theta)\n",
    "\n",
    "            # Ensure cases stay above zero for poisson\n",
    "            mu = pm.math.maximum(.1, expected_today)\n",
    "            observed = self.onset.round().values[1:]\n",
    "            cases = pm.Poisson('cases', mu=mu, observed=observed)\n",
    "\n",
    "            self.trace = pm.sample(\n",
    "                chains=chains,\n",
    "                tune=tune,\n",
    "                draws=draws,\n",
    "                target_accept=target_accept)\n",
    "            \n",
    "            return self\n",
    "    \n",
    "    def run_gp(self):\n",
    "        with pm.Model() as model:\n",
    "            gp_shape = len(self.onset) - 1\n",
    "\n",
    "            length_scale = pm.Gamma(\"length_scale\", alpha=3, beta=.4)\n",
    "\n",
    "            eta = .05\n",
    "            cov_func = eta**2 * pm.gp.cov.ExpQuad(1, length_scale)\n",
    "\n",
    "            gp = pm.gp.Latent(mean_func=pm.gp.mean.Constant(c=0), \n",
    "                              cov_func=cov_func)\n",
    "\n",
    "            # Place a GP prior over the function f.\n",
    "            theta = gp.prior(\"theta\", X=np.arange(gp_shape)[:, None])\n",
    "\n",
    "            # Let the serial interval be a random variable and calculate r_t\n",
    "            serial_interval = pm.Gamma('serial_interval', alpha=6, beta=1.5)\n",
    "            gamma = 1.0 / serial_interval\n",
    "            r_t = pm.Deterministic('r_t', theta / gamma + 1)\n",
    "\n",
    "            inferred_yesterday = self.onset.values[:-1] / self.cumulative_p_delay[:-1]\n",
    "            expected_today = inferred_yesterday * self.cumulative_p_delay[1:] * pm.math.exp(theta)\n",
    "\n",
    "            # Ensure cases stay above zero for poisson\n",
    "            mu = pm.math.maximum(.1, expected_today)\n",
    "            observed = self.onset.round().values[1:]\n",
    "            cases = pm.Poisson('cases', mu=mu, observed=observed)\n",
    "\n",
    "            self.trace = pm.sample(chains=1, tune=1000, draws=1000, target_accept=.8)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Pymc3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_model(model):\n",
    "    \n",
    "    r_t = model.trace['r_t']\n",
    "    mean = np.mean(r_t, axis=0)\n",
    "    median = np.median(r_t, axis=0)\n",
    "    hpd_90 = pm.stats.hpd(r_t, credible_interval=.9)\n",
    "    hpd_50 = pm.stats.hpd(r_t, credible_interval=.5)\n",
    "    \n",
    "    idx = pd.MultiIndex.from_product([\n",
    "            [model.region],\n",
    "            model.trace_index\n",
    "        ], names=['region', 'date'])\n",
    "        \n",
    "    df = pd.DataFrame(data=np.c_[mean, median, hpd_90, hpd_50], index=idx,\n",
    "                 columns=['mean', 'median', 'lower_90', 'upper_90', 'lower_50','upper_50'])\n",
    "    return df\n",
    "\n",
    "def create_and_run_model(name, county_state, case_col='new_cases'):\n",
    "    confirmed = county_state[case_col].dropna()\n",
    "    onset = confirmed_to_onset(confirmed, p_delay)\n",
    "    adjusted, cumulative_p_delay = adjust_onset_for_right_censorship(onset, p_delay)\n",
    "    return MCMCModel(name, onset, cumulative_p_delay).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rt(name, result, ax, c=(.3,.3,.3,1), ci=(0,0,0,.05)):\n",
    "    ax.set_ylim(0.5, 1.6)\n",
    "    ax.set_title(name)\n",
    "    ax.plot(result['median'],\n",
    "            marker='o',\n",
    "            markersize=4,\n",
    "            markerfacecolor='w',\n",
    "            lw=1,\n",
    "            c=c,\n",
    "            markevery=2)\n",
    "    ax.fill_between(\n",
    "        result.index,\n",
    "        result['lower_90'].values,\n",
    "        result['upper_90'].values,\n",
    "        color=ci,\n",
    "        lw=0)\n",
    "    ax.axhline(1.0, linestyle=':', lw=1)\n",
    "    \n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\n",
    "    ax.xaxis.set_major_locator(mdates.WeekdayLocator(interval=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Method: All Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_divergences (models):\n",
    "    ##################################\n",
    "\n",
    "    # Check to see if there were divergences\n",
    "    n_diverging = lambda x: x.trace['diverging'].nonzero()[0].size\n",
    "    divergences = pd.Series([n_diverging(m) for m in models.values()], index=models.keys())\n",
    "    has_divergences = divergences.gt(0)\n",
    "\n",
    "    print('Diverging states:')\n",
    "    display(divergences[has_divergences])\n",
    "\n",
    "    # Rerun counties with divergences\n",
    "    for county_state, n_divergences in divergences[has_divergences].items():\n",
    "        models[county_state].run(chains=2)\n",
    "\n",
    "    gc.collect()\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regional_rt_model(subset_df, \n",
    "                  region_col='County_State',\n",
    "                  date_col='date',\n",
    "                  case_col='new_cases',\n",
    "                  output_path=None,\n",
    "               ):\n",
    "\n",
    "    \n",
    "    ## Assuming no duplicates\n",
    "    ## Consider the scenario where we intend to calculate these results \n",
    "    ## at a state level, instead of at the county level.\n",
    "    \n",
    "    subset_df = subset_df.groupby([region_col, date_col])\\\n",
    "        [case_col].sum().reset_index().sort_values(date_col)\n",
    "    \n",
    "    ######################################\n",
    "    \n",
    "    models = {}\n",
    "    err_list = []\n",
    "    NUM_REGIONS = subset_df[region_col].nunique()\n",
    "    j = 0\n",
    "    for region, grp in subset_df.set_index(date_col).groupby(region_col):\n",
    "        \n",
    "        j = j+1\n",
    "        print (f'\\t\\t{j} of {NUM_REGIONS} regions in current subset...')\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            if region in models:\n",
    "                print(f'Skipping {region}, already in cache')\n",
    "                continue\n",
    "\n",
    "            models[region] = create_and_run_model(region, grp, case_col)\n",
    "        except:\n",
    "            err_list.append(region)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    ######################################\n",
    "    \n",
    "    # Check to see if there were divergences\n",
    "    n_diverging = lambda x: x.trace['diverging'].nonzero()[0].size\n",
    "    divergences = pd.Series([n_diverging(m) for m in models.values()], index=models.keys())\n",
    "    has_divergences = divergences.gt(0)\n",
    "\n",
    "    print('Diverging states:')\n",
    "    display(divergences[has_divergences])\n",
    "\n",
    "    # Rerun counties with divergences\n",
    "    for region, n_divergences in divergences[has_divergences].items():\n",
    "        models[region].run(chains=2)\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    ######################################\n",
    "\n",
    "    results = None\n",
    "\n",
    "    for region, model in models.items():\n",
    "\n",
    "        df = df_from_model(model)\n",
    "\n",
    "        if results is None:\n",
    "            results = df\n",
    "        else:\n",
    "            results = pd.concat([results, df], axis=0)\n",
    "\n",
    "    ##################################\n",
    "    \n",
    "    if output_path is not None:\n",
    "        results.to_csv(output_path)\n",
    "\n",
    "    return results, err_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_delay = calc_p_delay('../data/misc/latestdata.csv')\n",
    "state = 'AL'\n",
    "subset_df = cases_pop_df[cases_pop_df['State'].isin([state])]\n",
    "results, err_list = regional_rt_model(subset_df, case_col='new_cases',region_col='state', output_path=f'../../DATA/rt_state/rt_state_{state}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_delay = calc_p_delay('../data/misc/latestdata.csv')\n",
    "subset_df = cases_pop_df[cases_pop_df['state'].isin(['DE'])]\n",
    "results, err_list = regional_rt_model(subset_df, case_col='new_cases',region_col='state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 4\n",
    "nrows = int(np.ceil(results.index.levels[0].shape[0] / ncols))\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=nrows,\n",
    "    ncols=ncols,\n",
    "    figsize=(14, nrows*3),\n",
    "    sharey='row')\n",
    "\n",
    "for ax, (county_state, result) in zip(axes.flat, results.groupby('region')):\n",
    "    plot_rt(county_state, result.droplevel(0), ax)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor('w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
